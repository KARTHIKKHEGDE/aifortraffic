{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06434412",
   "metadata": {},
   "source": [
    "# üö¶ Traffic Signal RL Training (PPO)\n",
    "\n",
    "This notebook trains a **city-agnostic** reinforcement learning agent for traffic signal control.\n",
    "\n",
    "**Key Points:**\n",
    "- Uses synthetic traffic patterns (NOT real data)\n",
    "- PPO algorithm (stable, sample-efficient)\n",
    "- Can run on Colab with GPU acceleration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95444b5b",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install Dependencies\n",
    "\n",
    "Run this cell first (especially on Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install stable-baselines3[extra] gymnasium numpy pandas -q\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14f742",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Traffic Environment (City-Agnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generic Traffic Environment for RL Training\n",
    "============================================\n",
    "City-agnostic, 4-way intersection simulator.\n",
    "NO city names, NO geography - pure abstract traffic control.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from enum import IntEnum\n",
    "from typing import Tuple, Dict\n",
    "import random\n",
    "\n",
    "\n",
    "class Phase(IntEnum):\n",
    "    \"\"\"4-phase signal structure.\"\"\"\n",
    "    NS_GREEN = 0\n",
    "    NS_LEFT = 1\n",
    "    EW_GREEN = 2\n",
    "    EW_LEFT = 3\n",
    "\n",
    "\n",
    "class TrafficPatternGenerator:\n",
    "    \"\"\"Generates synthetic arrival rates for training.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def uniform_random(min_rate=0.1, max_rate=1.5):\n",
    "        return {d: np.random.uniform(min_rate, max_rate) \n",
    "                for d in ['north', 'south', 'east', 'west']}\n",
    "    \n",
    "    @staticmethod\n",
    "    def peak_hour(peak_direction=None):\n",
    "        if peak_direction is None:\n",
    "            peak_direction = random.choice(['north', 'south', 'east', 'west'])\n",
    "        rates = {d: np.random.uniform(0.2, 0.5) for d in ['north', 'south', 'east', 'west']}\n",
    "        rates[peak_direction] = np.random.uniform(1.0, 2.0)\n",
    "        return rates\n",
    "    \n",
    "    @staticmethod\n",
    "    def symmetric_flow():\n",
    "        ns_rate = np.random.uniform(0.3, 1.5)\n",
    "        ew_rate = np.random.uniform(0.3, 1.5)\n",
    "        return {'north': ns_rate, 'south': ns_rate, 'east': ew_rate, 'west': ew_rate}\n",
    "    \n",
    "    @staticmethod\n",
    "    def low_traffic():\n",
    "        return {d: np.random.uniform(0.05, 0.3) for d in ['north', 'south', 'east', 'west']}\n",
    "    \n",
    "    @staticmethod\n",
    "    def heavy_traffic():\n",
    "        return {d: np.random.uniform(1.0, 2.5) for d in ['north', 'south', 'east', 'west']}\n",
    "    \n",
    "    @staticmethod\n",
    "    def asymmetric_random():\n",
    "        rates = {}\n",
    "        for d in ['north', 'south', 'east', 'west']:\n",
    "            rates[d] = np.random.uniform(1.5, 3.0) if random.random() < 0.3 else np.random.uniform(0.1, 0.5)\n",
    "        return rates\n",
    "    \n",
    "    @classmethod\n",
    "    def get_random_pattern(cls):\n",
    "        patterns = [cls.uniform_random, cls.peak_hour, cls.symmetric_flow,\n",
    "                    cls.low_traffic, cls.heavy_traffic, cls.asymmetric_random]\n",
    "        return random.choice(patterns)()\n",
    "\n",
    "\n",
    "class TrafficEnv:\n",
    "    \"\"\"Generic 4-way intersection environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_steps=3600, min_green_time=10, max_green_time=60,\n",
    "                 yellow_time=3, saturation_flow=0.5, num_lanes=2, max_queue=100,\n",
    "                 arrival_rates=None):\n",
    "        self.max_steps = max_steps\n",
    "        self.min_green_time = min_green_time\n",
    "        self.max_green_time = max_green_time\n",
    "        self.yellow_time = yellow_time\n",
    "        self.saturation_flow = saturation_flow\n",
    "        self.num_lanes = num_lanes\n",
    "        self.max_queue = max_queue\n",
    "        self.directions = ['north', 'south', 'east', 'west']\n",
    "        \n",
    "        self.phase_to_directions = {\n",
    "            Phase.NS_GREEN: ['north', 'south'],\n",
    "            Phase.NS_LEFT: ['north', 'south'],\n",
    "            Phase.EW_GREEN: ['east', 'west'],\n",
    "            Phase.EW_LEFT: ['east', 'west'],\n",
    "        }\n",
    "        \n",
    "        self.queues = None\n",
    "        self.current_phase = None\n",
    "        self.phase_timer = None\n",
    "        self.step_count = None\n",
    "        self.total_waiting = None\n",
    "        self.arrival_rates = arrival_rates\n",
    "        self.n_actions = 5\n",
    "        self.state_dim = 9  # 4 queues + 4 phase one-hot + timer\n",
    "    \n",
    "    def reset(self, arrival_rates=None):\n",
    "        self.queues = {d: np.random.randint(0, 10) for d in self.directions}\n",
    "        self.current_phase = Phase(np.random.randint(0, 4))\n",
    "        self.phase_timer = np.random.randint(self.min_green_time, self.max_green_time)\n",
    "        self.step_count = 0\n",
    "        self.total_waiting = 0\n",
    "        self.total_throughput = 0\n",
    "        \n",
    "        if arrival_rates is not None:\n",
    "            self.arrival_rates = arrival_rates\n",
    "        elif self.arrival_rates is None:\n",
    "            self.arrival_rates = TrafficPatternGenerator.get_random_pattern()\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        queue_state = np.array([self.queues[d] / self.max_queue for d in self.directions])\n",
    "        phase_one_hot = np.zeros(4)\n",
    "        phase_one_hot[self.current_phase] = 1.0\n",
    "        timer_norm = np.array([self.phase_timer / self.max_green_time])\n",
    "        return np.concatenate([queue_state, phase_one_hot, timer_norm])\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        switched_phase = False\n",
    "        \n",
    "        # Force switch if timer expired\n",
    "        if self.phase_timer == 0:\n",
    "            self.current_phase = Phase((self.current_phase + 1) % 4)\n",
    "            self.phase_timer = self.min_green_time\n",
    "            switched_phase = True\n",
    "        \n",
    "        # Handle action\n",
    "        if not switched_phase:\n",
    "            if action == 0:\n",
    "                if self.phase_timer < self.max_green_time:\n",
    "                    self.phase_timer += 1\n",
    "            else:\n",
    "                new_phase = Phase(action - 1)\n",
    "                if new_phase != self.current_phase:\n",
    "                    self.current_phase = new_phase\n",
    "                    self.phase_timer = self.min_green_time\n",
    "                    switched_phase = True\n",
    "        \n",
    "        # Vehicle arrivals\n",
    "        for direction in self.directions:\n",
    "            arrivals = np.random.poisson(self.arrival_rates[direction])\n",
    "            self.queues[direction] = min(self.queues[direction] + arrivals, self.max_queue)\n",
    "        \n",
    "        # Vehicle departures\n",
    "        green_directions = self.phase_to_directions[self.current_phase]\n",
    "        for direction in green_directions:\n",
    "            departures = int(self.saturation_flow * self.num_lanes)\n",
    "            actual_departures = min(departures, self.queues[direction])\n",
    "            self.queues[direction] -= actual_departures\n",
    "            self.total_throughput += actual_departures\n",
    "        \n",
    "        self.phase_timer = max(0, self.phase_timer - 1)\n",
    "        \n",
    "        # Calculate reward\n",
    "        total_queue = sum(self.queues.values())\n",
    "        self.total_waiting += total_queue\n",
    "        reward = -total_queue / (self.max_queue * 4)\n",
    "        queue_std = np.std(list(self.queues.values()))\n",
    "        reward -= 0.1 * (queue_std / self.max_queue)\n",
    "        if switched_phase:\n",
    "            reward -= 0.2\n",
    "        \n",
    "        done = self.step_count >= self.max_steps\n",
    "        info = {\n",
    "            'queues': self.queues.copy(),\n",
    "            'phase': self.current_phase,\n",
    "            'total_waiting': self.total_waiting,\n",
    "            'throughput': self.total_throughput,\n",
    "            'avg_queue': total_queue / 4,\n",
    "            'reward': reward,\n",
    "            'total_queue': total_queue,\n",
    "            'switched': switched_phase,\n",
    "        }\n",
    "        \n",
    "        return self._get_state(), reward, done, info\n",
    "\n",
    "print(\"‚úÖ TrafficEnv defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847fd76",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Gymnasium Wrapper (for Stable-Baselines3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "class TrafficGymEnv(gym.Env):\n",
    "    \"\"\"Gymnasium-compatible wrapper for TrafficEnv.\"\"\"\n",
    "    \n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "    \n",
    "    def __init__(self, max_steps=3600, randomize_pattern=True):\n",
    "        super().__init__()\n",
    "        self.env = TrafficEnv(max_steps=max_steps)\n",
    "        self.randomize_pattern = randomize_pattern\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0.0, high=1.0, shape=(self.env.state_dim,), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = spaces.Discrete(self.env.n_actions)\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        pattern = TrafficPatternGenerator.get_random_pattern() if self.randomize_pattern else None\n",
    "        obs = self.env.reset(arrival_rates=pattern)\n",
    "        return obs.astype(np.float32), {\"arrival_rates\": self.env.arrival_rates}\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return obs.astype(np.float32), reward, done, False, info\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Quick test\n",
    "env = TrafficGymEnv(max_steps=100)\n",
    "obs, info = env.reset()\n",
    "print(f\"‚úÖ Gym wrapper ready!\")\n",
    "print(f\"   Observation shape: {obs.shape}\")\n",
    "print(f\"   Action space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6d14dd",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e760f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRAINING HYPERPARAMETERS\n",
    "# ============================================\n",
    "\n",
    "# Environment\n",
    "MAX_STEPS_PER_EPISODE = 3600    # 1 hour simulation\n",
    "N_ENVS = 4                       # Parallel environments\n",
    "\n",
    "# PPO Hyperparameters\n",
    "LEARNING_RATE = 3e-4\n",
    "N_STEPS = 2048                   # Steps per rollout\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 10                    # PPO epochs per update\n",
    "GAMMA = 0.99                     # Discount factor\n",
    "GAE_LAMBDA = 0.95               # GAE lambda\n",
    "CLIP_RANGE = 0.2                # PPO clip range\n",
    "ENT_COEF = 0.01                 # Entropy (exploration)\n",
    "\n",
    "# Training duration\n",
    "# üîß ADJUST THIS BASED ON YOUR TIME\n",
    "TOTAL_TIMESTEPS = 500_000       # ~15-30 min on Colab GPU\n",
    "\n",
    "print(\"‚úÖ Config set!\")\n",
    "print(f\"   Total timesteps: {TOTAL_TIMESTEPS:,}\")\n",
    "print(f\"   Parallel envs: {N_ENVS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b601590",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Create PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "import os\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Create vectorized environment\n",
    "def make_env():\n",
    "    env = TrafficGymEnv(max_steps=MAX_STEPS_PER_EPISODE, randomize_pattern=True)\n",
    "    return Monitor(env)\n",
    "\n",
    "train_env = DummyVecEnv([make_env for _ in range(N_ENVS)])\n",
    "eval_env = DummyVecEnv([make_env])\n",
    "\n",
    "print(f\"‚úÖ Environments created!\")\n",
    "print(f\"   Training envs: {N_ENVS}\")\n",
    "print(f\"   Observation space: {train_env.observation_space}\")\n",
    "print(f\"   Action space: {train_env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375fee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PPO agent\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=train_env,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_steps=N_STEPS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    gamma=GAMMA,\n",
    "    gae_lambda=GAE_LAMBDA,\n",
    "    clip_range=CLIP_RANGE,\n",
    "    ent_coef=ENT_COEF,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"logs/\",\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ PPO Agent created!\")\n",
    "print(f\"   Policy: MlpPolicy\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.policy.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a050e72f",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ea065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation callback - saves best model\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"models/best_model\",\n",
    "    log_path=\"logs/eval\",\n",
    "    eval_freq=10000 // N_ENVS,\n",
    "    n_eval_episodes=5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    ")\n",
    "\n",
    "# Checkpoint callback - periodic saves\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=50000 // N_ENVS,\n",
    "    save_path=\"models/checkpoints\",\n",
    "    name_prefix=\"ppo_traffic\",\n",
    ")\n",
    "\n",
    "callbacks = [eval_callback, checkpoint_callback]\n",
    "\n",
    "print(\"‚úÖ Callbacks ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576ac97",
   "metadata": {},
   "source": [
    "## üöÄ 7Ô∏è‚É£ TRAIN THE MODEL\n",
    "\n",
    "**This is the main training cell!**\n",
    "\n",
    "‚è±Ô∏è Expected time:\n",
    "- 500K steps: ~15-30 min (Colab GPU)\n",
    "- 1M steps: ~30-60 min (Colab GPU)\n",
    "\n",
    "You can interrupt with `Ctrl+C` and the model will still be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf35bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üöÄ STARTING PPO TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total timesteps: {TOTAL_TIMESTEPS:,}\")\n",
    "print(f\"This may take 15-30 minutes...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    model.learn(\n",
    "        total_timesteps=TOTAL_TIMESTEPS,\n",
    "        callback=callbacks,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Training interrupted!\")\n",
    "\n",
    "# Save final model\n",
    "model.save(\"models/ppo_traffic_final\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"üíæ Model saved to: models/ppo_traffic_final.zip\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e9188",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Test the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9518820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "best_model = PPO.load(\"models/best_model/best_model\")\n",
    "print(\"‚úÖ Best model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0abfd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on different traffic patterns\n",
    "test_patterns = [\n",
    "    (\"Low Traffic\", TrafficPatternGenerator.low_traffic()),\n",
    "    (\"Heavy Traffic\", TrafficPatternGenerator.heavy_traffic()),\n",
    "    (\"Peak Hour\", TrafficPatternGenerator.peak_hour()),\n",
    "    (\"Symmetric\", TrafficPatternGenerator.symmetric_flow()),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä TESTING TRAINED AGENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, rates in test_patterns:\n",
    "    env = TrafficGymEnv(max_steps=1000)\n",
    "    obs, _ = env.reset()\n",
    "    env.env.arrival_rates = rates  # Override with test pattern\n",
    "    \n",
    "    total_reward = 0\n",
    "    for _ in range(1000):\n",
    "        action, _ = best_model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Total Reward: {total_reward:.2f}\")\n",
    "    print(f\"  Avg Queue: {info['avg_queue']:.2f}\")\n",
    "    print(f\"  Throughput: {info['throughput']} vehicles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74544ef1",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Download Trained Model\n",
    "\n",
    "Run this to download your trained model (for Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb52b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab - download the model\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('models/ppo_traffic_final.zip')\n",
    "    print(\"‚úÖ Download started!\")\n",
    "except:\n",
    "    print(\"Not running on Colab. Model saved locally at: models/ppo_traffic_final.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eccf5dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Done!\n",
    "\n",
    "Your trained model is saved at:\n",
    "- `models/ppo_traffic_final.zip` - Final model\n",
    "- `models/best_model/best_model.zip` - Best during training\n",
    "\n",
    "**Next steps:**\n",
    "1. Download the model\n",
    "2. Evaluate on Silk Board arrival rates\n",
    "3. Compare with fixed-time baseline"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
